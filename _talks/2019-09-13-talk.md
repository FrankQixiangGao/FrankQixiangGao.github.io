---
title: "Rational Recurrences for Empirical Natural Language Processing"
collection: talks
type: "Talk"
permalink: /talks/2019-09-13-talk
venue: "Cornell University"
date: 2019-09-13
location: "New York"
---

<iframe width="560" height="315" src="https://www.youtube.com/embed/N0MEu2BSJc4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Despite their often-discussed advantages, deep learning methods largely disregard theories of both learning and language.  This makes their prediction behavior hard to understand and explain.  In this talk, I will present a path toward more understandable (but still "deep") natural language processing models, without sacrificing accuracy.  Rational recurrences comprise a family of recurrent neural networks that obey a particular set of rules about how to calculate hidden states, and hence correspond to parallelized weighted finite-state pattern matching.  Many recently introduced models turn out to be members of this family, and the weighted finite-state view lets us derive some new ones.  I'll introduce rational RNNs and present some of the ways we have used them in NLP.  My collaborators on this work include Jesse Dodge, Hao Peng, Roy Schwartz, and Sam Thomson.

